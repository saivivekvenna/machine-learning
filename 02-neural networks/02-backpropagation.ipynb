{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "\n",
    "Cost fuction: $ J(\\hat{y}, y) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}^i(\\hat{y}^i, y^i) $\n",
    "\n",
    "Binary cross-entropy loss: $ \\mathcal{L} = - [y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})] $, commonly used for binary classification tasks. This penalizes the model based on how far the predicted probability $ \\hat{y} $ is from the true label $ y $.\n",
    "\n",
    "Here is a good example walk through:\n",
    "\n",
    "**Goal for One Training Example (e.g., image of '2'):**\n",
    "Compute how this example \"wants\" to nudge all weights/biases to reduce cost.\n",
    "Outputs are initially random; desired: High activation for '2' neuron, low for others.\n",
    "Nudges proportional to distance from target (e.g., bigger nudge for far-off activations).\n",
    "\n",
    "**Backward Propagation Process:**\n",
    "\n",
    "1. Start at Output Layer: Increase '2' activation, decrease others (proportional to error).\n",
    "\n",
    "- Activation = Weighted sum of previous layer activations + bias, then squishified.\n",
    "To increase:\n",
    "\n",
    "2. Increase bias: Increase weights (bigger impact from connections to bright previous neurons). Change previous layer activations (brighten positives, dim negatives; proportional to weights).\n",
    "\n",
    "Note: \"Hebbian Analogy\": Strengthen connections between active neurons (\"neurons that fire together wire together\") – loose bio inspiration.\n",
    "\n",
    "Propagate Back: Each output neuron has \"desires\" for previous layer changes.\n",
    "Sum desires across all output neurons (weighted by how much each needs to change).\n",
    "Repeat recursively: Use these \"desired nudges\" for previous layer's weights/biases.\n",
    "\n",
    "**Averaging Across Examples:** One example alone would overfit. Compute nudges for all examples, average them → Negative gradient (or proportional to it).\n",
    "\n",
    "\n",
    "**Key**: Backprop layers \"nudges\" backward: Error from output → hidden layers, computing relative sensitivities."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
