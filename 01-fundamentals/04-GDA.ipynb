{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to GLM's\n",
    "\n",
    "Discriminate Learning Algorithim:\n",
    "- Goal: Learns $ P(y|x) $ directly—how to map features $ x $ to class $ y $\n",
    "- These are the features, is the tumor cancer or not?\n",
    "\n",
    "\n",
    "\n",
    "Generative Learning Algorithim:\n",
    "- Goal: Learns $ P(x|y) $ (how features look for each class) and $ P(y) $ (class priors), then uses $ P(y|x) = \\frac{P(x|y) P(y)}{P(x)} $ to predict.\n",
    "- Given the tumor is cancer, what are the features going to be like?\n",
    "\n",
    "**$ x $:** This is the input data or features. It’s the information you feed into the model to make a prediction. In your tumor example, $ x $ could be things like:\n",
    "\n",
    "Tumor size (e.g., 2 cm).\n",
    "Texture score (e.g., 0.8).\n",
    "Color intensity (e.g., 150).\n",
    "Basically, $ x $ is a vector of numbers describing the tumor’s characteristics.\n",
    "\n",
    "\n",
    "**$ y $:** This is the output or class/label. It’s what you’re trying to predict. In your case:\n",
    "\n",
    "$ y = 1 $ might mean \"tumor is cancer.\"\n",
    "$ y = 0 $ might mean \"tumor is not cancer.\"\n",
    "It’s the binary decision (or category) the model outputs.\n",
    "\n",
    "### Bayes Rule:\n",
    "Bayes Rule flips the relationship between $ P(y | x) $ and $ P(x | y) $\n",
    "\n",
    "$ P(y = 1|x) = \\frac{P(x|y = 1) P(y = 1)}{P(x)} $\n",
    "\n",
    "- $ P(y = 1 | x) $: This is the posterior probability. It’s the chance that the class is $ y = 1 $ (e.g., \"tumor is cancer\") given the features $ x $ (e.g., tumor size, texture). \n",
    "- $ P(x | y = 1) $: This is the likelihood. It’s the probability of seeing the features $ x $ if the class is $ y = 1 $ (e.g., how likely a tumor with size 2 cm and texture 0.8 is if it’s cancer). This is what generative models focus on.\n",
    "\n",
    "\n",
    "### Gasussian Discriminate Analysis (GDA)\n",
    "\n",
    "Suppose $  x \\in \\mathbb{R}   $ (drop the convention where $x_0 = 1$)\n",
    "Assume $### Intro to GLM's\n",
    "\n",
    "Discriminate Learning Algorithim:\n",
    "- Goal: Learns $ P(y|x) $ directly—how to map features $ x $ to class $ y $\n",
    "- These are the features, is the tumor cancer or not?\n",
    "- You want to fix gaussian to the postive and negative data points\n",
    "\n",
    "\n",
    "\n",
    "Generative Learning Algorithim:\n",
    "- Goal: Learns $ P(x|y) $ (how features look for each class) and $ P(y) $ (class priors), then uses $ P(y|x) = \\frac{P(x|y) P(y)}{P(x)} $ to predict.\n",
    "- Given the tumor is cancer, what are the features going to be like?\n",
    "\n",
    "**$ x $:** This is the input data or features. It’s the information you feed into the model to make a prediction. In your tumor example, $ x $ could be things like:\n",
    "\n",
    "Tumor size (e.g., 2 cm).\n",
    "Texture score (e.g., 0.8).\n",
    "Color intensity (e.g., 150).\n",
    "Basically, $ x $ is a vector of numbers describing the tumor’s characteristics.\n",
    "\n",
    "\n",
    "**$ y $:** This is the output or class/label. It’s what you’re trying to predict. In your case:\n",
    "\n",
    "$ y = 1 $ might mean \"tumor is cancer.\"\n",
    "$ y = 0 $ might mean \"tumor is not cancer.\"\n",
    "It’s the binary decision (or category) the model outputs.\n",
    "\n",
    "### Bayes Rule:\n",
    "Bayes Rule flips the relationship between $ P(y | x) $ and $ P(x | y) $\n",
    "\n",
    "$ P(y = 1|x) = \\frac{P(x|y = 1) P(y = 1)}{P(x)} $\n",
    "\n",
    "- $ P(y = 1 | x) $: This is the posterior probability. It’s the chance that the class is $ y = 1 $ (e.g., \"tumor is cancer\") given the features $ x $ (e.g., tumor size, texture). \n",
    "- $ P(x | y = 1) $: This is the likelihood. It’s the probability of seeing the features $ x $ if the class is $ y = 1 $ (e.g., how likely a tumor with size 2 cm and texture 0.8 is if it’s cancer). This is what generative models focus on.\n",
    "\n",
    "\n",
    "### Gasussian Discriminate Analysis (GDA)\n",
    "\n",
    "This is a machine learning technique used for classification tasks.\n",
    "\n",
    "GDA tries to classify data points into different categories by modeling each class with a bell-curve-like distribution. It uses the means ($ \\mu $) and covariance matrices ($ \\Sigma $) of each class to decide which category a new data point most likely belongs to.\n",
    "\n",
    "\n",
    "\n",
    "Suppose $  x \\in \\mathbb{R}   $ (drop the convention where $x_0 = 1$)\n",
    "Assume $ P(x|y) $ is Gaussian\n",
    "\n",
    "The PDF for Multi-Variable Distribution is: \n",
    "\n",
    "$$ \\phi(z) = \\frac{1}{(2\\pi)^{n/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right) $$\n",
    "\n",
    "This is a very common formula. \n",
    "\n",
    "The parameters are:\n",
    "$ \\mu_0 $ and $ \\mu_1 $: These are the mean vectors for two different classes (e.g., class 0 and class 1). Each vector contains the average values of the features for that class.\n",
    "$ \\Sigma $: This is the covariance matrix, which can be shared across classes in basic GDA or different for each class in QDA. It describes the spread and relationships of the features.\n",
    "\n",
    "**Training these parameters you need:**\n",
    "\n",
    "Training Set: $\\Sigma(x^i,y^i)_{i=1}^m$\n",
    "\n",
    "Joint Likelihood: \n",
    "\n",
    "$$ L(\\phi, \\mu, \\mu, \\Sigma) = \\prod_{i=1}^m p(x^{(i)}, y^{(i)} ; \\phi, \\mu, \\mu, \\Sigma) $$\n",
    "This says the joint likelihood is the product ($∏$) of the probabilities of each data point $ (x^{(i)}, y^{(i)}) $ from 1 to $ m $ (total number of data points). The biggest diffrence from discrimnate learning algorithms is the fact that we are maximizing both x and y as apose to just y. \n",
    "\n",
    "$ \\phi = \\frac{\\sum_{i=1}^m y^{(i)}}{m} = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = 1\\}}{m} $  This is very simple, just takes the overall probabilty of each class. This helps the model account for how common each class is in the data.The ${}$ repersent an indicator fuction where if its true its equal to one and false is equal to 0.\n",
    "\n",
    "$   \\mu_0 = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = 0\\} x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = 0\\}}   $ This is also simple, if you wanted the maximum likelihood estmiate for all the cancerous cells, you just look at all the cancerous cells in your training set and find the average. The numerator is just saying, if the indicator is true(1) then sum $1 \\times$features, if its false then its $0 \\times $features.The denominator is just the number of examples. Overall a simple mean calcuation. \n",
    "\n",
    "$   \\mu_1 = \\frac{\\sum_{i=1}^m 1\\{y^{(i)} = 1\\} x^{(i)}}{\\sum_{i=1}^m 1\\{y^{(i)} = 1\\}}   $ This is the same as $\\mu_0$ instead its just looking for the positive examples ($y=1$ as appose to $y=0$).\n",
    "\n",
    "$ \\Sigma = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)} - \\mu_{y^{(i)}})(x^{(i)} - \\mu_{y^{(i)}})^T $ This represents the spread and relationships of the features across all data points, regardless of class.\n",
    "\n",
    "### Prediction Rule\n",
    "\n",
    "$$ \\arg \\max_y p(y | x) = \\arg \\max_y \\frac{p(x | y) p(y)}{p(x)} $$\n",
    "\n",
    "$\\arg \\max_y$ means the number we have to plug in such that the equations results in the biggest value. \n",
    "\n",
    "### When to use GDA vs Regression\n",
    "\n",
    "GDA Assumptions (Generative, Stronger Assumptions)\n",
    "\n",
    "$ x | y = 0 \\sim \\mathcal{N}(\\mu_0, \\Sigma) $\n",
    "$ x | y = 1 \\sim \\mathcal{N}(\\mu_1, \\Sigma) $\n",
    "$ y \\sim \\text{Ber}(\\phi) $\n",
    "\n",
    "\n",
    "Comparison to Logistic Regression (Discriminative, Weaker Assumptions)\n",
    "\n",
    "$ p(y = 1 | x) = \\frac{1}{1 + e^{-(\\theta^T x + \\theta_0)}} $ (Logistic Function)\n",
    "\n",
    "The set of assumtions made in GDA implies the logistic fuction, but not in the oppsite direction. \n",
    "\n",
    "Key Difference\n",
    "\n",
    "GDA models the joint distribution $ p(x, y) $ and derives $ p(y | x) $. It also assumes the data is gaussian. \n",
    "Logistic regression directly models $ p(y | x) $ without assuming a specific distribution for $ x $.\n",
    "\n",
    "### Naive Bayes \n",
    "\n",
    "Suppose you are trying to orginize your email. How would you take the texts and catigorize it? This is a text classification problem.\n",
    "\n",
    "Firstly you want to assemble all the words you want to look out for. Might be the 10,000 most reoccuring words in your inbox. \n",
    "\n",
    "One way to do this is to create a binary feature vector that creates a 1 if a word appears in the email or 0 if not. \n",
    "\n",
    "$x ∈ \\{0,1\\}^n$ with the condition {word i appeares in email}\n",
    "\n",
    "In this example $n$ would be 10,000. \n",
    "\n",
    "**Key Idea:** Assume $ X_i $’s are conditionally independent given $ y $\": This means that once the class $ y $ (e.g., spam or not spam) is known, the features $ X_1, X_2, \\ldots, X_{n} $ (e.g., words in an email) don’t influence each other.\n",
    "\n",
    "$$\\prod_{i=1}^n p(X_i | y) $$\n",
    "\n",
    "## Implementation Of Naive Bayes\n",
    "\n",
    "I followed [this video](https://www.youtube.com/watch?v=TLInuAorxqE) to implement as python code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlstuff)",
   "language": "python",
   "name": "mlstuff"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
