{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to GLM's\n",
    "\n",
    "Discriminate Learning Algorithim:\n",
    "- Goal: Learns $ P(y|x) $ directly—how to map features $ x $ to class $ y $\n",
    "- These are the features, is the tumor cancer or not?\n",
    "\n",
    "\n",
    "\n",
    "Generative Learning Algorithim:\n",
    "- Goal: Learns $ P(x|y) $ (how features look for each class) and $ P(y) $ (class priors), then uses $ P(y|x) = \\frac{P(x|y) P(y)}{P(x)} $ to predict.\n",
    "- Given the tumor is cancer, what are the features going to be like?\n",
    "\n",
    "**$ x $:** This is the input data or features. It’s the information you feed into the model to make a prediction. In your tumor example, $ x $ could be things like:\n",
    "\n",
    "Tumor size (e.g., 2 cm).\n",
    "Texture score (e.g., 0.8).\n",
    "Color intensity (e.g., 150).\n",
    "Basically, $ x $ is a vector of numbers describing the tumor’s characteristics.\n",
    "\n",
    "\n",
    "**$ y $:** This is the output or class/label. It’s what you’re trying to predict. In your case:\n",
    "\n",
    "$ y = 1 $ might mean \"tumor is cancer.\"\n",
    "$ y = 0 $ might mean \"tumor is not cancer.\"\n",
    "It’s the binary decision (or category) the model outputs.\n",
    "\n",
    "### Bayes Rule:\n",
    "Bayes Rule flips the relationship between $ P(y | x) $ and $ P(x | y) $\n",
    "\n",
    "$ P(y = 1|x) = \\frac{P(x|y = 1) P(y = 1)}{P(x)} $\n",
    "\n",
    "- $ P(y = 1 | x) $: This is the posterior probability. It’s the chance that the class is $ y = 1 $ (e.g., \"tumor is cancer\") given the features $ x $ (e.g., tumor size, texture). \n",
    "- $ P(x | y = 1) $: This is the likelihood. It’s the probability of seeing the features $ x $ if the class is $ y = 1 $ (e.g., how likely a tumor with size 2 cm and texture 0.8 is if it’s cancer). This is what generative models focus on.\n",
    "\n",
    "\n",
    "### Gasussian Discriminate Analysis (GDA)\n",
    "\n",
    "Suppose $  x \\in \\mathbb{R}   $ (drop the convention where $x_0 = 1$)\n",
    "Assume $### Intro to GLM's\n",
    "\n",
    "Discriminate Learning Algorithim:\n",
    "- Goal: Learns $ P(y|x) $ directly—how to map features $ x $ to class $ y $\n",
    "- These are the features, is the tumor cancer or not?\n",
    "\n",
    "\n",
    "\n",
    "Generative Learning Algorithim:\n",
    "- Goal: Learns $ P(x|y) $ (how features look for each class) and $ P(y) $ (class priors), then uses $ P(y|x) = \\frac{P(x|y) P(y)}{P(x)} $ to predict.\n",
    "- Given the tumor is cancer, what are the features going to be like?\n",
    "\n",
    "**$ x $:** This is the input data or features. It’s the information you feed into the model to make a prediction. In your tumor example, $ x $ could be things like:\n",
    "\n",
    "Tumor size (e.g., 2 cm).\n",
    "Texture score (e.g., 0.8).\n",
    "Color intensity (e.g., 150).\n",
    "Basically, $ x $ is a vector of numbers describing the tumor’s characteristics.\n",
    "\n",
    "\n",
    "**$ y $:** This is the output or class/label. It’s what you’re trying to predict. In your case:\n",
    "\n",
    "$ y = 1 $ might mean \"tumor is cancer.\"\n",
    "$ y = 0 $ might mean \"tumor is not cancer.\"\n",
    "It’s the binary decision (or category) the model outputs.\n",
    "\n",
    "### Bayes Rule:\n",
    "Bayes Rule flips the relationship between $ P(y | x) $ and $ P(x | y) $\n",
    "\n",
    "$ P(y = 1|x) = \\frac{P(x|y = 1) P(y = 1)}{P(x)} $\n",
    "\n",
    "- $ P(y = 1 | x) $: This is the posterior probability. It’s the chance that the class is $ y = 1 $ (e.g., \"tumor is cancer\") given the features $ x $ (e.g., tumor size, texture). \n",
    "- $ P(x | y = 1) $: This is the likelihood. It’s the probability of seeing the features $ x $ if the class is $ y = 1 $ (e.g., how likely a tumor with size 2 cm and texture 0.8 is if it’s cancer). This is what generative models focus on.\n",
    "\n",
    "\n",
    "### Gasussian Discriminate Analysis (GDA)\n",
    "\n",
    "Suppose $  x \\in \\mathbb{R}   $ (drop the convention where $x_0 = 1$)\n",
    "Assume $ P(x|y) $ is Gaussian\n",
    "\n",
    "The PDF for Multi-Variable Distribution is: \n",
    "\n",
    "$$ \\phi(z) = \\frac{1}{(2\\pi)^{n/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right) $$\n",
    "\n",
    "This is a very common formula. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
