{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally Weighted and Logistic Regression\n",
    "\n",
    "I followed [Andrew Ng's CS229 Lecture 3](https://www.youtube.com/watch?v=het9HFqo1TQ) for this section. \n",
    "\n",
    "### My Notes\n",
    "\n",
    "**Locally weighted regression:**\n",
    "\n",
    "Parametic learning algorithm - fixed set of paraters, like linear regression(LR)\n",
    "\n",
    "Non-parametric learning algorithm - amount of data/ parameters increases (linearly) with the data set. \n",
    "\n",
    "To evaluate H and a certain value of x:\n",
    "\n",
    "- LR: Fit theta to minimizes cost fuction. \n",
    "- Locally weighted regression: Focusing on only the training exmaples which are near the point you want to predict. This is the forumla:\n",
    "\n",
    "$J(\\theta) = \\sum_{i=1}^n w^{(i)} (y^{(i)} - \\theta^T x^{(i)})^2$\n",
    "\n",
    "where:\n",
    "- $\\theta$: The parameter vector to be optimized.\n",
    "- $x^{(i)}$: The $i$-th training input vector.\n",
    "- $y^{(i)}$: The $i$-th training output value.\n",
    "- $w^{(i)}$: The weight for the $i$-th training example\n",
    "  $w^{(i)} = \\exp\\left(-\\frac{\\|x^{(i)} - x\\|^2}{2\\tau^2}\\right)$\n",
    "- $\\tau$: The bandwidth parameter controlling the locality of the weighting.\n",
    "- $n$: The number of training examples.\n",
    "\n",
    "For the weight formula notice: \n",
    "\n",
    "If $|x^{(i)} - x|$ is small then $w^{(i)}$ will be close to 1.\n",
    "\n",
    "If $|x^{(i)} - x|$ is big then $w^{(i)}$ will be close to 0.\n",
    "\n",
    "So essentally what the weight is doing is ensuring the examples close to the point you want to predict are being counted by being mulitiped by 1 and those far away and not as important to the calcuation thus being multiplied by close to 0. \n",
    "\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "Returns a binary result.\n",
    "\n",
    "We want $h_\\theta(x) \\in [0,1]$ \n",
    "\n",
    "In logistic regression, the hypothesis $h_\\theta(x)$ represents the predicted probability that the output $y = 1$ given input features $x$. It is defined as:\n",
    "\n",
    "$$h_\\theta(x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}}$$\n",
    "\n",
    "where:\n",
    "- $x$ is the input feature vector (including a bias term $x_0 = 1$).\n",
    "- $\\theta$ is the parameter vector (weights).\n",
    "- $\\theta^T x = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n$ is the linear combination of features.\n",
    "- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function, ensuring $h_\\theta(x) \\in [0, 1]$.\n",
    "\n",
    "The sigmond fuction has an asymtote at 0 and at 1, with the graph starting on the left near 0 and increasing to the y-int of 0.1 then approaching 1. \n",
    "\n",
    "The data will have this type of distubution:\n",
    "\n",
    "${P(y = 1 | x; \\theta)} = h_\\theta(x)$ \n",
    "${P(y = 0 | x; \\theta)} = 1 - h_\\theta(x)$\n",
    "\n",
    "but remember since this is a a method of binary classification, y can only be $\\{0,1\\}$. So the two assumtions we made for the type of distubution can be compressed into one:\n",
    "\n",
    "${P(y | x; \\theta)} = h(x)^y(1-h(x))^{1-y}$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
