{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression and Gradient Descent\n",
    "\n",
    "I followed [Andrew Ng's CS229 Video Lecture 2](https://www.youtube.com/watch?v=4b4MUYve_U8) for this section. Hands down one of the best and comprensive lectures on this subject with a fantastic teacher. \n",
    "\n",
    "### My Notes\n",
    "\n",
    "Linear Regression is one of the most common statistical and machine learning algorithm. It is used to find a linear relationship between one or more predictors. Their are two types of linear regression, simple regression and multiple regression (MLR). The algorithm allows a computer to predict future data. \n",
    "\n",
    "**Simple Linear Regression:**\n",
    "\n",
    "A model with a **single independent variable** (predictor) and a **single dependent variable** (outcome). Simple regression is made up of four basic assumtions. \n",
    "\n",
    "1. There is a linear relationship between predictor and outcome. \n",
    "2. The variation around the regression line is constant. The average variation from all the points is the same. \n",
    "3. The vartiation of data points around the regression line follows a normal distribution. This means that majority of the data points will be closer to regression line and less points will be father away. If you plot the variation from the regression line it should form a bell curve. \n",
    "4. The deviation of each point is independent of the other points. The value of devation of one point has no relation to other points deviation. \n",
    "\n",
    "\n",
    "**Supervised Learning:**\n",
    "\n",
    "Dataset -> Learning Algorithm -> Hypothesis\n",
    "\n",
    "New Data -> Hypothesis -> Prediction \n",
    "\n",
    "Hypothesis:  $h(x) = \\theta_0 + \\theta_1  x$\n",
    "\n",
    "- Let $h(x)$ be the output \n",
    "- Let $\\theta_0$ be z-int\n",
    "- Let $\\theta_1$ be the slope (weight of feature)\n",
    "- Let $x$ be the input \n",
    "\n",
    "The hypothesis's formula is nothing but a normal linear equation, think about in the sense of $y = mx+b$.\n",
    "\n",
    "If you have more than one input features you can edit the formula to match the new parameteres. \n",
    "\n",
    "The learning models job is to find a $\\theta$ value that allows you to create accurate predictions. \n",
    "\n",
    "\n",
    "Hypothesis:  $h(x) = \\theta_0 + \\theta_1  x_1 + \\theta_2  x_2$\n",
    "\n",
    "- Let $h(x)$  be the output \n",
    "- Let $\\theta_0$ be z-int\n",
    "- Let $\\theta_1$ be the slope (weight of feature 1)\n",
    "- Let $\\theta_2$ be the slope (weight of feature 2)\n",
    "- Let $x_1$ be the input of feature 1\n",
    "- Let $x_2$ be the input of feature 2\n",
    "\n",
    "So basicly you are just adding the weight and input of each new feature to the equation.\n",
    "\n",
    "Important Definitions:\n",
    "- Let $m$ be # training examples (rows on the dataset)\n",
    "- Let $(x^i, y^i)$ be training example at index $i$\n",
    "- Let $n$ be number of features for the learning model\n",
    "\n",
    "You can compact this hypothesis more into:\n",
    "$$ h(x) = \\theta_0 + \\sum_{j=1}^n \\theta_j x_j $$\n",
    "\n",
    "Or you can introduce a dummy feature where $x_0 = 1$ and write the formula as: \n",
    "$$h(x) =\\sum_{j=0}^n \\theta_j x_j$$\n",
    "\n",
    "So how would you figure out what to set $\\theta$ to? At its most basic, choose a $\\theta$ that would most closely make $h(x) = y$ for the training examples. \n",
    "\n",
    "\n",
    "\n",
    "**Least Squares Method:**\n",
    "The least squared method is a way to find the $\\theta_0$ and $\\theta_1$ (z-int and slope) of the regression line. \n",
    "\n",
    "Residual is the diffrence between the actual and the predicted z value. Residual = Actual - Predicted. A negative residual is when the acutal is below the predicted and a positive is when it is above the predicited. This is useful to tell us how far off our regression line is from the actual data. \n",
    "\n",
    "Now one can say, if you were to just add up all the residuals of the data and find the smallest one, your regression line would be the most accurate. This wouldn't work since you have both negative and positive values. The most common method to this approach is to find the sum of all the squares of the residuals. This way they would all be positive and would show a proportionally accurate line. \n",
    "\n",
    "The formula for the least squares method is:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h(x^i) - y^i)^2$$\n",
    "\n",
    "All this is finding is that the sum of the squared diffrence between the output and the actual (test for each example case). The goal is for it to be the smallest error. \n",
    "\n",
    "**Gradient Descent:**\n",
    "Best way of thinking about this is to plot $J(\\theta)$ on the Z axis, and $\\theta_0$ and $\\theta_1$ on the x and y axis. \n",
    "\n",
    "![https://miro.medium.com/v2/1*f9a162GhpMbiTVTAua_lLQ.png](https://miro.medium.com/v2/1*f9a162GhpMbiTVTAua_lLQ.png)\n",
    "\n",
    "Then the lowest points in this graph would repersent the $\\theta_0$ and $\\theta_1$ that allow for the lowest error. \n",
    "\n",
    "The way gradient descent work is that first it starts off at an random point on the graphs, checks in every direction for the direction it needs to move in inorder to go downhill and repeats this untill it find the lowest most point. This lowest point is the local optima. \n",
    "\n",
    "Now something that might raise an alarm is that it might be very easy for this algorithm to get caught in a local optima thinking it is the most optimized but in reality it is not. In linear regression there will not be a local optimum, only global. Its really just a quadratic fuction and is thus much simpler. The graph would look like this:\n",
    "\n",
    "![https://machinelearningspace.com/wp-content/uploads/2023/01/Gradient-Descent-Top2-1024x645.png](https://machinelearningspace.com/wp-content/uploads/2023/01/Gradient-Descent-Top2-1024x645.png)\n",
    "\n",
    "The steps that graident descent repeats are descibed as follows:\n",
    "\n",
    "$(\\theta_j) (where (j = 0, 1, \\dots, n))$ is:\n",
    "\n",
    "$[ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) ]$\n",
    "\n",
    "Where:\n",
    "- $\\theta_j$: The $j$-th parameter (e.g., $\\theta_0$ for the intercept, $\\theta_1, \\theta_2, \\dots$ for feature weights).\n",
    "- $\\alpha$: The learning rate\n",
    "- $\\frac{\\partial}{\\partial \\theta_j} J(\\theta)$: The partial derivative of the cost function $J(\\theta)$ with respect to $\\theta_j$, representing the gradient (direction and magnitude of the steepest increase in $J$).\n",
    "- $:=$ Indicates that $\\theta_j$ is updated with the new value.\n",
    "\n",
    "This keeps going until the there is no longer a smaller value. \n",
    "\n",
    "**Batch Gradient Descent:**\n",
    "\n",
    "Batch Gradient Descent is what we are currently doing where we iterate through each example case. This practical situations this might not be the best methods since you might have very large datasets. \n",
    "\n",
    "**Normal Equation**\n",
    "\n",
    "Allows you to jump straight to the global optima - ONLY WORKS WITH LINEAR REGRESSION\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Implementation\n",
    "\n",
    "Followed [this video](https://www.youtube.com/watch?v=Jj7WD71qQWE) by Harry Connor AI.\n",
    "\n",
    "1. Cost Fuction using Least Squares Method\n",
    "2. Gradient Fuction to get derivatives of cost function\n",
    "3. Gradient Descent Fuction to reiterate and find continuity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # this libary is for any numberical calcuations\n",
    "import pandas as pd # this is to load the data set\n",
    "import matplotlib.pyplot as plt # plot data and show regression line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv(\"../references/datasets/salary_data.csv\") # reading the dataset. it has a single input function. \n",
    "\n",
    "x_train = training_set[\"YearsExperience\"].values #independant variable/ input function\n",
    "y_train = training_set[\"Salary\"].values # dependant variable/ output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 9449.9623, b: 25792.2002\n"
     ]
    }
   ],
   "source": [
    "def cost_function(x ,y, w, b):  # w and b are just theta 0 and 1 (int and slope)\n",
    "    m = len(x) # number of training examples\n",
    "    cost_sum = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f = w*x[i]+b\n",
    "        cost = (f - y[i]) ** 2 # getting the squared error\n",
    "        cost_sum += cost\n",
    "\n",
    "    total_cost = (1/(2*m))  * cost_sum\n",
    "    return total_cost\n",
    "\n",
    "def gradient_function(x, y, w, b):\n",
    "    m = len(x)\n",
    "    dc_dw = 0\n",
    "    dc_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f = w*x[i]+b\n",
    "\n",
    "        dc_dw += (f-y[i])*x[i]\n",
    "        dc_db += (f-y[i])\n",
    "\n",
    "    dc_dw = (1/m) * dc_dw\n",
    "    dc_db = (1/m) * dc_db\n",
    "\n",
    "    return dc_dw, dc_db\n",
    "\n",
    "def gradient_descent(x,y, alpha, iterations):\n",
    "    w = 0\n",
    "    b = 0 \n",
    "\n",
    "    for i in range(iterations):\n",
    "        dc_dw, dc_db = gradient_function(x,y,w,b)\n",
    "\n",
    "        w = w - alpha*dc_dw\n",
    "        b = b - alpha*dc_db\n",
    "\n",
    "        #print(f\"Interation {i}: Cost {cost_function(x,y,w,b)}\")\n",
    "\n",
    "    return w, b \n",
    "\n",
    "learning_rate = 0.01\n",
    "iterations = 10000\n",
    "\n",
    "final_w, final_b = gradient_descent(x_train, y_train, learning_rate, iterations)\n",
    "\n",
    "print(f\"w: {final_w:.4f}, b: {final_b:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
