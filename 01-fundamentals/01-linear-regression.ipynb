{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Linear Regression is one of the most common statistical and machine learning algorithm. It is used to find a linear relationship between one or more predictors. Their are two types of linear regression, simple regression and multiple regression (MLR). The algorithm allows a computer to predict future data. \n",
    "\n",
    "#### Simple Linear Regression:\n",
    "A model with a **single independent variable** (predictor) and a **single dependent variable** (outcome). Simple regression is made up of four basic assumtions. \n",
    "\n",
    "1. There is a linear relationship between predictor and outcome. \n",
    "2. The variation around the regression line is constant. The average variation from all the points is the same. \n",
    "3. The vartiation of data points around the regression line follows a normal distribution. This means that majority of the data points will be closer to regression line and less points will be father away. If you plot the variation from the regression line it should form a bell curve. \n",
    "4. The deviation of each point is independent of the other points. The value of devation of one point has no relation to other points deviation. \n",
    "\n",
    "### My Notes\n",
    "\n",
    "**Supervised Learning:**\n",
    "\n",
    "Dataset -> Learning Algorithm -> Hypothesis\n",
    "\n",
    "New Data -> Hypothesis -> Prediction \n",
    "\n",
    "Hypothesis:  $h(x) = \\theta_0 + \\theta_1  x$\n",
    "\n",
    "- Let $h(x)$ be the output \n",
    "- Let $\\theta_0$ be z-int\n",
    "- Let $\\theta_1$ be the slope (weight of feature)\n",
    "- Let $x$ be the input \n",
    "\n",
    "The hypothesis's formula is nothing but a normal linear equation, think about in the sense of $y = mx+b$.\n",
    "\n",
    "If you have more than one input features you can edit the formula to match the new parameteres. \n",
    "\n",
    "The learning models job is to find a $\\theta$ value that allows you to create accurate predictions. \n",
    "\n",
    "\n",
    "Hypothesis:  $h(x) = \\theta_0 + \\theta_1  x_1 + \\theta_2  x_2$\n",
    "\n",
    "- Let $h(x)$  be the output \n",
    "- Let $\\theta_0$ be z-int\n",
    "- Let $\\theta_1$ be the slope (weight of feature 1)\n",
    "- Let $\\theta_2$ be the slope (weight of feature 2)\n",
    "- Let $x_1$ be the input of feature 1\n",
    "- Let $x_2$ be the input of feature 2\n",
    "\n",
    "So basicly you are just adding the weight and input of each new feature to the equation.\n",
    "\n",
    "Important Definitions:\n",
    "- Let $m$ be # training examples (rows on the dataset)\n",
    "- Let $(x^i, y^i)$ be training example at index $i$\n",
    "- Let $n$ be number of features for the learning model\n",
    "\n",
    "You can compact this hypothesis more into:\n",
    "$$ h(x) = \\theta_0 + \\sum_{j=1}^n \\theta_j x_j $$\n",
    "\n",
    "Or you can introduce a dummy feature where $x_0 = 1$ and write the formula as: \n",
    "$$h(x) =\\sum_{j=0}^n \\theta_j x_j$$\n",
    "\n",
    "So how would you figure out what to set $\\theta$ to? At its most basic, choose a $\\theta$ that would most closely make $h(x) = y$ for the training examples. \n",
    "\n",
    "\n",
    "\n",
    "**Least Squares Method:**\n",
    "The least squared method is a way to find the $\\theta_0$ and $\\theta_1$ (z-int and slope) of the regression line. \n",
    "\n",
    "Residual is the diffrence between the actual and the predicted z value. Residual = Actual - Predicted. A negative residual is when the acutal is below the predicted and a positive is when it is above the predicited. This is useful to tell us how far off our regression line is from the actual data. \n",
    "\n",
    "Now one can say, if you were to just add up all the residuals of the data and find the smallest one, your regression line would be the most accurate. This wouldn't work since you have both negative and positive values. The most common method to this approach is to find the sum of all the squares of the residuals. This way they would all be positive and would show a proportionally accurate line. \n",
    "\n",
    "The formula for the least squares method is:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h(x^i) - y^i)^2$$\n",
    "\n",
    "All this is finding is that the sum of the squared diffrence between the output and the actual (test for each example case). The goal is for it to be the smallest error. \n",
    "\n",
    "**Gradient Descent:**\n",
    "Best way of thinking about this is to plot $J(\\theta)$ on the Z axis, and $\\theta_0$ and $\\theta_1$ on the x and y axis. \n",
    "\n",
    "![https://miro.medium.com/v2/1*f9a162GhpMbiTVTAua_lLQ.png](https://miro.medium.com/v2/1*f9a162GhpMbiTVTAua_lLQ.png)\n",
    "\n",
    "Then the lowest points in this graph would repersent the $\\theta_0$ and $\\theta_1$ that allow for the lowest error. \n",
    "\n",
    "The way gradient descent work is that first it starts off at an random point on the graphs, checks in every direction for the direction it needs to move in inorder to go downhill and repeats this untill it find the lowest most point. This lowest point is the local optima. \n",
    "\n",
    "Now something that might raise an alarm is that it might be very easy for this algorithm to get caught in a local optima thinking it is the most optimized but in reality it is not. \n",
    "\n",
    "Note: In linear regression there will not be a local optimum, only global. Its really just a quadratic fuction and is thus much simpler. The graph would look like this:\n",
    "\n",
    "![https://machinelearningspace.com/wp-content/uploads/2023/01/Gradient-Descent-Top2-1024x645.png](https://machinelearningspace.com/wp-content/uploads/2023/01/Gradient-Descent-Top2-1024x645.png)\n",
    "\n",
    "$(\\theta_j) (where (j = 0, 1, \\dots, n))$ is:\n",
    "\n",
    "$[ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) ]$\n",
    "\n",
    "Where:\n",
    "- $\\theta_j$: The $j$-th parameter (e.g., $\\theta_0$ for the intercept, $\\theta_1, \\theta_2, \\dots$ for feature weights).\n",
    "- $\\alpha$: The learning rate\n",
    "- $\\frac{\\partial}{\\partial \\theta_j} J(\\theta)$: The partial derivative of the cost function $J(\\theta)$ with respect to $\\theta_j$, representing the gradient (direction and magnitude of the steepest increase in $J$).\n",
    "- $:=$ Indicates that $\\theta_j$ is updated with the new value.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
