{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "The perceptron at the very basics is simlar to a sigmoid fuction in the sense where its used for binary classification. The formula is:\n",
    "\n",
    "$$g(z) = \n",
    "\\begin{cases} \n",
    "1 & z \\geq 0 \\\\\n",
    "0 & z < 0 \n",
    "\\end{cases}$$\n",
    "\n",
    "The hypothesis is:\n",
    "\n",
    "$$h_\\theta(\\mathbf{x}) =  \n",
    "\\begin{cases} \n",
    "1 & \\text{if } \\mathbf{w} \\cdot \\mathbf{x} + b \\geq 0 \\\\\n",
    "0 & \\text{if } \\mathbf{w} \\cdot \\mathbf{x} + b < 0 \n",
    "\\end{cases}$$\n",
    "\n",
    "In essence, this hypothesis outputs 1 for the positive class and 0 for the negative class. This fuction creates a hard threshold at 0 unlike the sigmoid function. \n",
    "\n",
    "This model fails to give a probabilistic interpretation. :(\n",
    "\n",
    "### Exponential Family\n",
    "\n",
    "The exponential family is like a master template for tons of probability distributions. \n",
    "\n",
    "**PDF (Probabilty Density Fuction):** PDF is like a map that tells you where the data is more likely to be. It’s a curve that shows how “dense” the probability is at different spots. \n",
    "\n",
    "$$P(y|\\eta) = b(y) \\exp(\\eta^T T(y) - a(\\eta))$$\n",
    "\n",
    "where:\n",
    "- $y$ is the Data\n",
    "- $η$ is the natural param\n",
    "- $T(y)$ is the sufficent stat (in our context always equal to $y$)\n",
    "- $b(y)$ is the base measure  \n",
    "- $a(\\eta)$ is the log-partition\n",
    "\n",
    "**Bernouill (Binary 1 or 0)** \n",
    "\n",
    "Bernoulli PDF/PMF: For a binary variable $ y $ (0 or 1) with probability $ p $ of being 1:\n",
    "$$P(y | p) = p^y (1 - p)^{1 - y}$$\n",
    "\n",
    "If $ y = 1 $, it’s $ p $.\n",
    "If $ y = 0 $, it’s $ 1 - p $.\n",
    "\n",
    "\n",
    "Exponential Family Form: We can rewrite it to match $ P(y|\\eta) = b(y) \\exp(\\eta^T T(y) - a(\\eta)) $:\n",
    "\n",
    "$ y $ is the data (0 or 1).\n",
    "$ \\eta = \\log\\left(\\frac{p}{1-p}\\right) $, the natural parameter (log-odds, linking to logistic regression).\n",
    "$ T(y) = y $ (as you noted, the sufficient statistic is just $ y $ itself for Bernoulli).\n",
    "$ b(y) = 1 $ (the base measure is constant since $ y $ is 0 or 1).\n",
    "$ a(\\eta) = \\log(1 + e^\\eta) $, the log-partition function that normalizes it.\n",
    "\n",
    "\n",
    "After plugging in:\n",
    "$$\\frac{1}{1 + e^\\eta} $$\n",
    "This matches the Bernoulli form when you relate $ \\eta $ to $ p $ via the sigmoid: $ p = \\frac{e^\\eta}{1 + e^\\eta} $.\n",
    "\n",
    "The Bernoulli distribution in the exponential family lets logistic regression give probabilities (via the sigmoid) instead of the Perceptron’s hard 0/1. The exponential form makes it easy to tweak and optimize with math (like gradients).\n",
    "\n",
    "**Gaussian (Fixed Variance)**\n",
    "\n",
    "Gaussian is just a normal bell curve (standard deviation). \n",
    "\n",
    "PDF:\n",
    "$$P(y | \\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "$ \\mu $ is the mean, and $\\sigma^2$ is the fixed variance.\n",
    "\n",
    "\n",
    "Exponential Family Form:\n",
    "\n",
    "$ y $ is the data (any number).\n",
    "$ \\eta = \\frac{\\mu}{\\sigma^2} $, the natural parameter.\n",
    "$ T(y) = (y, y^2) $, the sufficient statistic (a vector with $ y $ and $ y^2 $).\n",
    "$ b(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} $, the base measure.\n",
    "$ a(\\eta) = \\frac{\\eta^2 \\sigma^2}{2} + \\text{constant} $, the log-partition function.\n",
    "\n",
    "\n",
    "Handles continuous data, not just binary like Bernoulli.\n",
    "\n",
    "### GLMS (Generalize Linear Models)\n",
    "\n",
    "**Key assumptions:**\n",
    "**(i)** $ y|x;0$ is part of the exponential family - This says that the output $ y $ (like whether something is 0 or 1, or a continuous value) depends on the input $ x $ and some parameters $ \\theta $, and it follows an exponential family distribution with a natural parameter $ \\eta $.\n",
    "\n",
    "**(ii)** $\\eta =\\theta^Tx$ - The natural parameter $ \\eta $ is calculated as the dot product of the parameter vector $ \\theta $ (transposed to $ \\theta^T $) and the input vector $ x $.\n",
    "\n",
    "**(iii)** Test time: Output $ E[y|x; \\theta] \\Rightarrow h_\\theta(x) = E[y|x; \\theta] $ - At test time (when you use the model to predict), the output is the expected value of $ y $ given $ x $ and $ \\theta $, written as $ E[y|x; \\theta] $. This expected value is also the hypothesis function $ h_\\theta(x) $.\n",
    "\n",
    "**GLM Training**\n",
    "\n",
    "No matter the exponential family we use, the learning update rule will always be the same. All you have to do is plug in the respective $h_\\theta(x^{(i)})$ We have seen this same rule many times already: \n",
    "\n",
    "$$\\theta_j := \\theta_j + \\alpha (y^{(i)} - h_\\theta(x^{(i)})) x_j^{(i)}$$\n",
    "\n",
    "Key Terms:\n",
    "- $ \\eta $ - natural parameter\n",
    "- $ E[y|\\eta] = g(\\eta)$ - Canonical Response\n",
    "- $ \\eta = g^{-1}(\\mu)$ - Canonical Link Function\n",
    "\n",
    "We have three types of **parameterizations**:\n",
    "\n",
    "Model Param ($ \\theta $)\n",
    "Natural Param ($ \\eta $)\n",
    "Canonical Param:\n",
    "- $\\phi $ - Bernouill \n",
    "- $ \\mu \\sigma^2 $ - Gamma \n",
    "- $ \\lambda $ - Poisson\n",
    "\n",
    "So when ever we are leanring a GLM, it the the model param ($ \\theta $) that we are actually learning. \n",
    "\n",
    "Model param's relationship to natual param is characterized by $\\eta =\\theta^Tx$. Between natural param and canonical param is characterized by $g$ or $g^{-1}$\n",
    " \n",
    "Something interesting is that if you follow this process and you use the Bernouill distbution, logisitic regression is the natural result. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
